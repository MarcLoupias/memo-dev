(window.webpackJsonp=window.webpackJsonp||[]).push([[29],{197:function(e,t,a){"use strict";a.r(t);var s=a(0),n=Object(s.a)({},(function(){var e=this,t=e._self._c;return t("div",{staticClass:"content"},[e._m(0),e._v(" "),e._m(1),e._v(" "),t("p",[t("a",{attrs:{href:"https://www.linkedin.com/feed/update/urn:li:activity:7133567569684238336/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Yann LeCun about IA training on LinkedIn"),t("OutboundLink")],1)]),e._v(" "),e._m(2),t("p",[e._v("TLDR : Next gen IA needs to use video instead of text.")]),e._v(" "),t("p",[e._v("To compare, see "),t("a",{attrs:{href:"https://www.youtube.com/watch?v=Kv4FzAdxclA",target:"_blank",rel:"noopener noreferrer"}},[e._v("this Jean-Baptiste Kempf (VLC) interview about how video works"),t("OutboundLink")],1),e._v(".")]),e._v(" "),e._m(3),e._v(" "),t("p",[e._v("Each CODEC behave the same way, they delete data not seen by eyes, and they seek data blocks that are redundant image by image or between images.")]),e._v(" "),e._m(4),t("ul",[t("li",[e._v("H.264 is the most common CODEC used in the world, around 80% of usage.")]),e._v(" "),t("li",[e._v("HEVC is crippled by royalties, it remains unused on the web instead of television, around 5%.")]),e._v(" "),t("li",[e._v("VP9 created by Google, royalty free, opensource, Youtube and Facebook uses it.")]),e._v(" "),t("li",[e._v("AV1 then AV2 created by the Open Media Alliance initiated by Google.")]),e._v(" "),t("li",[e._v("AV1 is implemented by "),t("a",{attrs:{href:"https://github.com/videolan/dav1d",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dav1d"),t("OutboundLink")],1),e._v(", a VLC project, around 210K assembly LoC + 30K C LoC. This impl is widely used by GAFAM.")])]),e._v(" "),e._m(5),e._v(" "),t("p",[t("a",{attrs:{href:"https://gen-ai.fr/outils/generation-code/chatgpt-pour-developpeurs/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Guide ChatGPT pour d√©veloppeurs"),t("OutboundLink")],1)])])}),[function(){var e=this._self._c;return e("h1",{attrs:{id:"ia"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#ia"}},[this._v("#")]),this._v(" IA")])},function(){var e=this._self._c;return e("h2",{attrs:{id:"training"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#training"}},[this._v("#")]),this._v(" training")])},function(){var e=this,t=e._self._c;return t("div",{staticClass:"language-text line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("Animals and humans get very smart very quickly with vastly smaller amounts of training data than current AI systems.\n\nCurrent LLMs are trained on text data that would take 20,000 years for a human to read.\nAnd still, they haven't learned that if A is the same as B, then B is the same as A.\nHumans get a lot smarter than that with comparatively little training data.\nEven corvids, parrots, dogs, and octopuses get smarter than that very, very quickly, with only 2 billion neurons and a few trillion \"parameters.\"\n\nMy money is on new architectures that would learn as efficiently as animals and humans.\nUsing more text data (synthetic or not) is a temporary stopgap made necessary by the limitations of our current approaches.\nThe salvation is in using sensory data, e.g. video, which has higher bandwidth and more internal structure.\n\nThe total amount of visual data seen by a 2 year-old is larger than the amount of data used to train LLMs, but still pretty reasonable.\n2 years = 2x365x12x3600 or roughly 32 million seconds.\nWe have 2 million optical nerve fibers, carrying roughly ten bytes per second each.\nThat's a total of 6E14 bytes. The volume of data for LLM training is typically 1E13 tokens, which is about 2E13 bytes.\nIt's a factor of 30.\n\nImportantly, there is more to learn from video than from text because it is more redundant.\nIt tells you a lot about the structure of the world.\n")])]),e._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[e._v("1")]),t("br"),t("span",{staticClass:"line-number"},[e._v("2")]),t("br"),t("span",{staticClass:"line-number"},[e._v("3")]),t("br"),t("span",{staticClass:"line-number"},[e._v("4")]),t("br"),t("span",{staticClass:"line-number"},[e._v("5")]),t("br"),t("span",{staticClass:"line-number"},[e._v("6")]),t("br"),t("span",{staticClass:"line-number"},[e._v("7")]),t("br"),t("span",{staticClass:"line-number"},[e._v("8")]),t("br"),t("span",{staticClass:"line-number"},[e._v("9")]),t("br"),t("span",{staticClass:"line-number"},[e._v("10")]),t("br"),t("span",{staticClass:"line-number"},[e._v("11")]),t("br"),t("span",{staticClass:"line-number"},[e._v("12")]),t("br"),t("span",{staticClass:"line-number"},[e._v("13")]),t("br"),t("span",{staticClass:"line-number"},[e._v("14")]),t("br"),t("span",{staticClass:"line-number"},[e._v("15")]),t("br"),t("span",{staticClass:"line-number"},[e._v("16")]),t("br"),t("span",{staticClass:"line-number"},[e._v("17")]),t("br"),t("span",{staticClass:"line-number"},[e._v("18")]),t("br"),t("span",{staticClass:"line-number"},[e._v("19")]),t("br")])])},function(){var e=this,t=e._self._c;return t("ul",[t("li",[e._v("an image is an array of pixel, each pixel is a color")]),e._v(" "),t("li",[e._v("a video is a collection of images (something between 24 to 60 images per second)")]),e._v(" "),t("li",[e._v("CODEC = compression decompression algorithm to send video.")]),e._v(" "),t("li",[e._v("Video pixel by pixel is around 10 to 40 Gb/s")]),e._v(" "),t("li",[e._v("the goal of CODEC is to divide 100, 200, ... 1K the bandwith used.")]),e._v(" "),t("li",[e._v("dividing bandwith is destroying information")]),e._v(" "),t("li",[e._v("the tech behind is based on how the human eyes behave, some colors are better seen then others, so we can delete some colors without downgrading the image seen.")])])},function(){var e=this._self._c;return e("div",{staticClass:"language-text line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[this._v("MPEG-1 (1993) ---\x3e MPEG-2 (1995) = DVD ---\x3e DIVX (1999) (=MPEG-4) ---\x3e H.264 (2003) ---\x3e HEVC (2013) ---\x3e VP9 (2013)\n")])]),this._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[this._v("1")]),e("br")])])},function(){var e=this._self._c;return e("h2",{attrs:{id:"misc"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#misc"}},[this._v("#")]),this._v(" misc")])}],!1,null,null,null);t.default=n.exports}}]);